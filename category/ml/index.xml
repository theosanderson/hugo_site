<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ML | Theo Sanderson</title><link>/category/ml/</link><atom:link href="/category/ml/index.xml" rel="self" type="application/rss+xml"/><description>ML</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 13 Nov 2018 14:51:57 +0000</lastBuildDate><image><url>/images/icon_hu141e5746c4390abcaff97a6bd8371d9c_4466_512x512_fill_lanczos_center_2.png</url><title>ML</title><link>/category/ml/</link></image><item><title>BigGAN interpolations</title><link>/blog/2018/11/13/biggan-interpolations/</link><pubDate>Tue, 13 Nov 2018 14:51:57 +0000</pubDate><guid>/blog/2018/11/13/biggan-interpolations/</guid><description>&lt;p>The state of the art in image generation is &lt;a href="https://arxiv.org/abs/1809.11096" target="_blank" rel="noopener">BigGAN&lt;/a>.&lt;/p>
&lt;p>Now, some &lt;a href="https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb" target="_blank" rel="noopener">trained models&lt;/a> have been made available, including the capacity to interpolate between classes. I made &lt;a href="https://colab.research.google.com/drive/1MhfEAOBwhGu1A-F2NSVxGQrkJ4vk7w4V#scrollTo=dSAyfDfnVugs&amp;amp;forceEdit=true&amp;amp;offline=true&amp;amp;sandboxMode=true" target="_blank" rel="noopener">a colab&lt;/a> to easily create animations from these.&lt;/p>
&lt;p>They are pretty fun.&lt;/p>
&lt;p>&lt;img class="aligncenter size-full wp-image-263" src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/11/cat.gif" alt="" width="256" height="256" />&lt;/p>
&lt;p>What is more, they make it clear that the latent space clearly captures very meaningful shared properties across classes. The poses of quite different animals are conserved, and “cat eyes” clearly map onto “dog eyes” during interpolation. These sort of properties suggest that the network ‘understands’ the scene it is generating.&lt;/p>
&lt;p>Here are some more:&lt;/p>
&lt;p>&lt;img class="aligncenter size-full wp-image-278" src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/11/cattohusky.gif" alt="" width="256" height="256" />&lt;/p>
&lt;p>&lt;img class="aligncenter size-full wp-image-280" src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/11/leopard.gif" alt="" width="256" height="256" />&lt;/p>
&lt;p>&lt;img class="aligncenter size-large wp-image-264" src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/11/goose2.gif" alt="" width="256" height="256" />&lt;/p>
&lt;p>&lt;img class="aligncenter size-large wp-image-267" src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/11/dog.gif" alt="" width="256" height="256" />&lt;/p>
&lt;p>&lt;img class="aligncenter" src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/11/panda2.gif" alt="" />&lt;/p>
&lt;p>&lt;img class="aligncenter size-large wp-image-267" src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/11/tiger.gif" alt="" />&lt;/p>
&lt;p>(this one moves in latent space as well as class space, hence the change of pose:)&lt;br>
&lt;img class="aligncenter size-large wp-image-267" src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/11/bulldog.gif" alt="" />&lt;br>
Churches to mosques:&lt;/p>
&lt;center>
&lt;img src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/11/mosque2.gif" alt="" width="256" height="256" /> &lt;img src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/11/mosque.gif" alt="" width="256" height="256" />
&lt;/center>
&lt;center>
&lt;/center>
&lt;center>
&lt;/center>
&lt;center>
&lt;/center></description></item><item><title>Adventures with InfoGANs: towards generative models of biological images (part 2)</title><link>/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images-part-2/</link><pubDate>Tue, 02 Oct 2018 10:05:06 +0000</pubDate><guid>/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images-part-2/</guid><description>&lt;p>In &lt;a href="http://theo.io/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images/" target="_blank" rel="noopener">the last post&lt;/a> I introduced neural networks, generative adversarial networks (GANs) and InfoGANs.&lt;/p>
&lt;p>In this post I’ll describe the motivation and strategy for creating a GAN which generates images of biological cells, like this:&lt;/p>
&lt;p>&lt;strong>Motivation&lt;/strong>&lt;br>
Two microscopic of cells from an identical cell-line are never going to be pixel-for-pixel identical, yet they still share key similarities in terms of morphology. One way to get a computer to demonstrate it “understands” these morphological properties is to force it to create images of new cells. When a model is capable of generating any image that might be taken of a specific cell one might argue that it has gained some knowledge about the cell.&lt;/p>
&lt;p>This is an appealing approach as it is is easy to acquire large numbers of static images of cells, for instance by using an imaging flow cytometer. This device flows cells past a camera and acquires thousands of images each second, each with a large number of fluorescent channels. These are static images of course, but if they come from an synchronous population they should contain representatives of every stage in the cell cycle. If our generative model was sufficiently capable of truly understanding the structure in the data (and there is &lt;a href="https://twitter.com/ericjang11/status/1046124500369047553" target="_blank" rel="noopener">evidence that such models do&lt;/a>) then provided if we could generate a model where one dimension in our generator corresponded to pseudotime we could generate timecourse “videos” from models trained on these single images. This would, apart from anything else, have practical utility, avoiding the problems of photobleaching in imaging.&lt;/p>
&lt;p>&lt;strong>What I did&lt;/strong>&lt;/p>
&lt;p>From my work at the &lt;a href="https://www.sanger.ac.uk/science/groups/rayner-group" target="_blank" rel="noopener">Rayner lab&lt;/a> at the Sanger Institute I have access to a large dataset of images of infected red blood cells acquired with an imaging flow cytometer. These images contain both a brightfield transmitted light channel and a fluorescent channel showing DNA in the parasite. I used some filtering (with a classification network) to isolate only images containing single cells which were fully visible. I then trained an InfoGAN in much the same way as in the digit dataset described in the last post.&lt;/p>
&lt;p>I tried a couple of different versions, one with a large number of “communicated” values – which is what you see above.&lt;/p>
&lt;p>The network learns about quite a few aspects of &lt;em>Plasmodium&lt;/em>&lt;br>
biology, and also some basic optics:&lt;/p>
&lt;ul>
&lt;li>It learns about red blood cell morphology and to produce images of plausible cells&lt;/li>
&lt;li>It learns that the nucleus in the DAPI channel is always within the bounds of the RBC in the brightfield channel.&lt;/li>
&lt;li>It learns that there are a subset of cells (schizonts) which have both black haemozoin, and widespread, bright DAPI nuclei (often arranged in a circular shape)&lt;/li>
&lt;li>It learns that cells can appear in front of or behind the focal plane and how to render both types.&lt;/li>
&lt;li>I could go on.. the images produced by the network are almost all entirely plausible images that a biologist would be unable to distinguish from true parasites.&lt;/li>
&lt;/ul>
&lt;p>As an aside I really like watching this network cycle between samples, it is strikingly similar to watching a motile cell wriggling:&lt;br>
&lt;img src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/09/cell_gan.gif" alt="" width="480" height="160" class="alignnone size-full wp-image-215" />&lt;/p>
&lt;p>I also trained a &lt;a href="http://theo.io/GAN/plasmodium_gan2/test.html" target="_blank" rel="noopener">second version&lt;/a>, with many fewer communicated variables, which really lets us see what the network sees as the most salient features. Unsurprisingly, these are mostly about the location and orientation of the red blood cell. Interestingly this network makes one of the parameters the focal position of the cell:&lt;/p>
&lt;p>&lt;img src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/09/focus.gif" alt="" width="480" height="232" class="alignnone size-full wp-image-216" />&lt;/p>
&lt;p>For any cell you look at, one end of this variable produces one type of halo, and the other the opposite, just as if this parameter was controlling the focus of a microscope.&lt;/p>
&lt;p>This property illustrates the sort of property it would be really exciting to get the network to learn. In an ideal world the sliders would represent something like: focus, cell orientation and parasite lifecycle stage. So if you had a young ring stage parasite and you wanted to watch it mature you could just drag the slider across. Or if you wanted to focus on the far side of the cell you could just drag another slider. Such an idea may sound fanciful, but I don’t think it’s too far away.&lt;/p>
&lt;p>There are a number of simple ways I could improve the work I’ve done here. The network has to capture the complete distribution of cellular images presented as “real”. In this case you can see that these are in many different orientations, and different positions in the image. So a vast amount of the networks efoort goes into recapturing that, not very interesting, spatial variation. I made some basic attempts to align the images presented to the network, but this is something I could definitely improve. Something you will see as the cells dance from one space in the network to another is that while the nucleus tends to move smoothly around the black haemozoin will often fade out in one location and appear in another. The lack of smoothness here is an example of “modal collapse” that often haunts GANs, but there are a number of ways to tackle it.&lt;/p>
&lt;p>In short, this work just scratches the surface of what what I suspect is possible with generative models of biological cells.&lt;/p>
&lt;p>In some organisms there are genome-wide fluorescent-tag libraries available. Building a generative model using these (possibly with the need for some pairwise imaging) could allow the creation of a synthetic cell in which every protein can be simultaneously visualised. It’s an exciting prospect, and I think it’s nearer than it seems.&lt;/p>
&lt;p>&lt;small>P.S. I belatedly looked for similar published work, and found two &lt;a href="https://www.biorxiv.org/content/early/2017/12/02/227645">cool&lt;/a> &lt;a href="https://arxiv.org/pdf/1708.04692.pdf">papers&lt;/a>. The second of these introduces a star-shaped network designed to allow alignment in much the way I imagine at the end of this post. And more generally there are a ton of GAN papers applying the technique to super-resolution microscopy, in silico staining, etc.&lt;/small>&lt;/p></description></item><item><title>Adventures with InfoGANs: towards generative models of biological images (part 1)</title><link>/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images/</link><pubDate>Tue, 02 Oct 2018 09:00:46 +0000</pubDate><guid>/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images/</guid><description>&lt;p>I recently began an &lt;a href="https://ai.google/research/join-us/ai-residency/" target="_blank" rel="noopener">AI Residency&lt;/a> at Google, which I am enjoying a great deal. I have been experimenting with deep-learning approaches for a few years now, but am excited to immerse myself in this world over the coming year. Biology increasingly generates very large datasets and I am convinced that novel machine-learning approaches will be essential to make the most of them.&lt;/p>
&lt;p>At the beginning of my residency, I was advised to complete a &lt;em>mini-project&lt;/em> which largely reimplements existing work, as an introduction to new tools.  In this post I’m going to describe what I got up to during that first few weeks, which culminated in the tool below that conjures up new images of red blood cells infected with malaria parasites:&lt;/p>
&lt;p>&lt;script>
function resizeIframe(obj) {
obj.style.height = obj.contentWindow.document.body.scrollHeight + 'px';
}
&lt;/script>&lt;br />
&lt;iframe src="http://static.theo.io/GAN/plasmodium_gan/test.html" style="width:100%; height:500px" frameborder="0" scrolling="no" onload="resizeIframe(this)" >&lt;/iframe>&lt;/p>
&lt;p>I’m going to try to explain it from first principles, which might take a while, so if you have a background in machine-learning, &lt;a href="http://theo.io/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images-part-2/" target="_blank" rel="noopener">skip to the next post&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Neural net-what?&lt;/strong>&lt;/p>
&lt;p>Deep learning is based around things called neural networks. A neural network takes a series of numbers in, does some processing to them, and ultimately spits out another set of numbers. A system like this is in theory capable of undertaking a great many tasks: for example the first series of numbers could be the brightness values for each pixel in an image, and the number outputted could be the probability that the image contains a cat. If the network can map those sets of numbers to each other successfully, then you have a cat-recognition network.&lt;/p>
&lt;figure id="attachment_221" aria-describedby="caption-attachment-221" style="width: 833px" class="wp-caption aligncenter">&lt;img src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/09/cat-01.png" alt="" width="833" height="342" class="size-full wp-image-221" srcset="/post/archive_posts/old_wp_images/wp-content/uploads/2018/09/cat-01.png 833w, /post/archive_posts/old_wp_images/wp-content/uploads/2018/09/cat-01-300x123.png 300w, /post/archive_posts/old_wp_images/wp-content/uploads/2018/09/cat-01-768x315.png 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px" />&lt;figcaption id="caption-attachment-221" class="wp-caption-text">A hypothetical classification network to detect images of cats&lt;/figcaption>&lt;/figure>
&lt;p>But what determines whether the network will recognise cats,  koalas, or fail to recognise anything at all? Each network contains a vast array of &lt;em>parameters&lt;/em>, which determine how the input numbers are processed to produce the output. These are knobs which can be twiddled to make the network do different things.&lt;/p>
&lt;p>The magic that makes neural networks powerful is a process called &lt;em>back-propagation&lt;/em>, which allows the system to automatically twiddle these knobs until the network produces the desired output for each input. This requires a large amount of labelled data. For our cat network we might give it 1,000 photos of cats, and 1,000 photos of dogs. The system will start off by setting the parameters to random values. It will then feed in the first cat photo and see what number it gets out. With these random values, the network will most likely output 0.5 as the cat probability, even though the true value is 1 (it’s a cat!). But with back-propagation the system can observe this error (0.5 – 1 = -0.5) and calculate which direction it needs to turn each knob (and by how much) to make the error smaller. If we repeat this process over and over and over again (which is called &lt;em>training&lt;/em>), we will eventually we find we have a network that can reliably distinguish cats and dogs. And it turns out that the graphics cards in our computers can be commandeered to make that process happen pretty quickly. In an hour or so we can have our cat recognition network – jubilations.&lt;/p>
&lt;p>With those basic principles one can build a network to do any sort of classification problem. If you have searched your own photos based on what they contain  (e.g. on Google photos) then you have interacted with just such a network.&lt;/p>
&lt;p>&lt;strong>Making computers creative&lt;/strong>&lt;/p>
&lt;p>You might think at this point that our network has a pretty good idea what a cat looks like, but unfortunately with this approach you can’t ask it to draw one.&lt;/p>
&lt;p>One &lt;em>can&lt;/em> imagine a neural network which could draw, however: this time the &lt;em>output&lt;/em> would be a series of numbers, which we will convert into to the brightness values of pixels in an image. What should the input be? It turns out we can just use random numbers:  for each image we can sample say 10 values from a normal distribution, and feed them in. That way, in theory, the network can draw on this randomness to generate a different picture each time, and act as an image &lt;em>generator&lt;/em>.&lt;/p>
&lt;p>There is a problem of course. The network still has no idea what cats look like. When it is created, with its knobs set to random positions, it will generate images that resemble those on an un-tuned TV. In theory we could train it ourselves, by letting it know whenever it produced noise that had the faintest of resemblance to a cat, until it was coaxed into doing what we wanted. In practice however, this would take an eternity.&lt;/p>
&lt;p>So what to do? Well, there’s a trick, &lt;a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">invented&lt;/a> just four years ago, which has been revolutionary. We can introduce a second network, called a &lt;em>discriminator&lt;/em>. This is a classification network, like the one we imagined above. But this time its task is not to distinguish cats from dogs, but to distinguish real photos of actual cats, from fake cats imagined by the generator. We feed it some real cats, and some fake ‘cats’ that the generator has produced, and we train it. Initially it has a very easy job here, since remember the generator is just displaying the images of an untuned TV. But wait..&lt;/p>
&lt;p>Once the discriminator starts to do a decent job we can take our &lt;em>back-propagation&lt;/em> one step further. The system can backpropagate from the final result (“fake” or “real”) back to the pixel values produced by the generator, and then all the way back through the generator to the random variables that made these fake images. Then it can ask &lt;em>“what directions should I turn the knobs of the generator so that it produces an output that the discriminator thinks is real?”&lt;/em>. In this way, the generator can be trained to fool the discriminator, and in doing so, the images it produces will become a bit more cat-like.&lt;/p>
&lt;p>The discriminator can now be trained again, to be more expert in telling these synthetic cats from their real counterparts. In fact we bounce back and forth, training generator and then discriminator in a continual loop. They have opposing goals, the generator wants to make realistic cat images and the discriminator wants to tell these apart from &lt;em>true&lt;/em> cat images, and so this architecture is called a &lt;em>generative adversarial network&lt;/em> or GAN. After a long period of training these networks can produce  results like these:&lt;/p>
&lt;figure id="attachment_180" aria-describedby="caption-attachment-180" style="width: 288px" class="wp-caption alignnone">&lt;img class=" wp-image-180" src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/08/Screenshot-from-2018-08-27-21-19-48.png" alt="" width="288" height="293" srcset="/post/archive_posts/old_wp_images/wp-content/uploads/2018/08/Screenshot-from-2018-08-27-21-19-48.png 663w, /post/archive_posts/old_wp_images/wp-content/uploads/2018/08/Screenshot-from-2018-08-27-21-19-48-296x300.png 296w" sizes="(max-width: 288px) 85vw, 288px" />&lt;figcaption id="caption-attachment-180" class="wp-caption-text">Results of a cat GAN. (This figure comes from a [paper][4] by Alexia Jolicoeur-Martinea about a novel type of discriminator)&lt;/figcaption>&lt;/figure>
&lt;figure id="attachment_179" aria-describedby="caption-attachment-179" style="width: 512px" class="wp-caption alignnone">&lt;img class="wp-image-179 size-full" src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/08/representative_image_512x256.png" alt="" width="512" height="256" srcset="/post/archive_posts/old_wp_images/wp-content/uploads/2018/08/representative_image_512x256.png 512w, /post/archive_posts/old_wp_images/wp-content/uploads/2018/08/representative_image_512x256-300x150.png 300w" sizes="(max-width: 512px) 85vw, 512px" />&lt;figcaption id="caption-attachment-179" class="wp-caption-text">And with some extensions of the GAN approach [Nvidia][5] were able to hallucinate these wholly imaginary celebrities.&lt;/figcaption>&lt;/figure>
&lt;p>Not bad, eh?&lt;/p>
&lt;p>&lt;strong>InfoGAN – images that communicate information&lt;/strong>&lt;/p>
&lt;p>This approach can produce realistic images. But the relationship between the random noise at the beginning and the image at the end is not generally clear. If we increase value 3 by 50% what will happen? It is generally difficult to predict, and does not correspond in a useful way to a semantically meaningful property of the image.&lt;/p>
&lt;p>There would be a lot of value to a network which, without being specifically trained to, could actually understand the structure in these images (e.g. that there are different breeds of cat) and that at the end could generate an image of any breed of your choice.&lt;/p>
&lt;p>Some people came up with a very clever way of doing this, called an InfoGAN. This approach is very similar to the original GAN. The generator still draws on random noise to produce its image. However the generator is also given some &lt;em>extra noise&lt;/em>, which it is tasked with encoding into the image it produces. For instance in the original InfoGAN paper the researchers produced a network which made images of hand-written digits. The extra noise they added was a discrete ‘one-hot’ variable with ten possible values. The idea was to create a network where each of these values corresponded to a different digit. To get this to happen they created an additional training objective for both the generator and the discriminator. As before, the discriminator still wants to distinguish real images from fake images and the generator still wants to fool the discriminator. However &lt;em>both the discriminator and the generator&lt;/em> are also rewarded if the discriminator is able to successfully reconstruct the extra random variable that was fed to the generator.&lt;/p>
&lt;p>This means the generator now has to encode &lt;em>information&lt;/em> (a variable with ten possible values) into an image, but that image has to be a plausible member of the set of hand-written digit images. The strategy that it ends up taking is to encode each value as a separate digit, and tada, we have achieved our goal. We can now ask the network to generate any specific digit we like. We can choose threes and generate an infinite number of different threes in different styles. The researchers also showed that if they added two more continuous variables to be communicated, these ended up mapped to the angle, and the width, of the digit produced.&lt;/p>
&lt;p> &lt;/p>
&lt;figure id="attachment_190" aria-describedby="caption-attachment-190" style="width: 639px" class="wp-caption alignnone">&lt;img class="wp-image-190 size-full" src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/09/1_kyyjNnuNaOscjucBpql2AA.png" alt="" width="639" height="390" srcset="/post/archive_posts/old_wp_images/wp-content/uploads/2018/09/1_kyyjNnuNaOscjucBpql2AA.png 639w, /post/archive_posts/old_wp_images/wp-content/uploads/2018/09/1_kyyjNnuNaOscjucBpql2AA-300x183.png 300w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px" />&lt;figcaption id="caption-attachment-190" class="wp-caption-text">Figure from the InfoGAN paper showing the three properties the network has learnt (digit type, angle and width)&lt;/figcaption>&lt;/figure>
&lt;p>I began by reimplementing some of what these researchers had achieved, focusing on this hand-written digit task. Here are the images my network produced evolving over time:&lt;/p>
&lt;p>&lt;img src="/post/archive_posts/old_wp_images/wp-content/uploads/2018/09/image3.gif" alt="" width="140" height="168" class="aligncenter size-full wp-image-223" />&lt;/p>
&lt;p>You can see the network gradually deciding how it will encode each value. It is undecided about what should be a five and what should be a zero for a long time, but it eventually comes to a conclusion. So without labelling any of the data, the system has learnt to partition it into ten appropriate categories, and has built a system for arbitrarily generation members of each of these categories.&lt;/p>
&lt;p>There are a near-infinite number of possible threes that can be drawn – cleaner or curlier, thicker or thinner, but all are united in being threes. I think that a very similar property applies in biological cells and in the &lt;a href="http://theo.io/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images-part-2/" target="_blank" rel="noopener">next post&lt;/a> I’ll describe my forays into this area, and the promise I think such an approach holds.&lt;/p>
&lt;p>&lt;a href="http://theo.io/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images-part-2/" target="_blank" rel="noopener">&amp;raquo;&amp;raquo; Part 2&lt;/a>&lt;/p></description></item></channel></rss>