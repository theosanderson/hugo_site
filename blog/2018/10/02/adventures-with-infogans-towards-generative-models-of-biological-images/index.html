<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.1 for Hugo"><meta name=author content="Theo Sanderson"><meta name=description content="I recently began an AI Residency at Google, which I am enjoying a great deal. I have been experimenting with deep-learning approaches for a few years now, but am excited to immerse myself in this world over the coming year."><link rel=alternate hreflang=en-us href=/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload="this.media='all'" disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous media=print onload="this.media='all'"><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload="this.media='all'"><link rel=stylesheet href=/css/wowchemy.a2dda909fb8228e17412d06b4423e0a6.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-65298-19"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
function trackOutboundLink(url,target){gtag('event','click',{'event_category':'outbound','event_label':url,'transport_type':'beacon','event_callback':function(){if(target!=='_blank'){document.location=url;}}});console.debug("Outbound link clicked: "+url);}
function onClickCallback(event){if((event.target.tagName!=='A')||(event.target.host===window.location.host)){return;}
trackOutboundLink(event.target,event.target.getAttribute('target'));}
gtag('js',new Date());gtag('config','UA-65298-19',{});document.addEventListener('click',onClickCallback,false);</script><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_hu141e5746c4390abcaff97a6bd8371d9c_4466_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/images/icon_hu141e5746c4390abcaff97a6bd8371d9c_4466_192x192_fill_lanczos_center_2.png><link rel=canonical href=/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@theosanderson"><meta property="twitter:creator" content="@theosanderson"><meta property="og:site_name" content="Theo Sanderson"><meta property="og:url" content="/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images/"><meta property="og:title" content="Adventures with InfoGANs: towards generative models of biological images (part 1) | Theo Sanderson"><meta property="og:description" content="I recently began an AI Residency at Google, which I am enjoying a great deal. I have been experimenting with deep-learning approaches for a few years now, but am excited to immerse myself in this world over the coming year."><meta property="og:image" content="/images/icon_hu141e5746c4390abcaff97a6bd8371d9c_4466_512x512_fill_lanczos_center_2.png"><meta property="twitter:image" content="/images/icon_hu141e5746c4390abcaff97a6bd8371d9c_4466_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2018-10-02T09:00:46+00:00"><meta property="article:modified_time" content="2018-10-02T09:00:46+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images/"},"headline":"Adventures with InfoGANs: towards generative models of biological images (part 1)","datePublished":"2018-10-02T09:00:46Z","dateModified":"2018-10-02T09:00:46Z","author":{"@type":"Person","name":"Theo Sanderson"},"publisher":{"@type":"Organization","name":"Theo Sanderson","logo":{"@type":"ImageObject","url":"/images/icon_hu141e5746c4390abcaff97a6bd8371d9c_4466_192x192_fill_lanczos_center_2.png"}},"description":"I recently began an AI Residency at Google, which I am enjoying a great deal. I have been experimenting with deep-learning approaches for a few years now, but am excited to immerse myself in this world over the coming year."}</script><title>Adventures with InfoGANs: towards generative models of biological images (part 1) | Theo Sanderson</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.7264cf0eba3b66951b36da7d2cecf9c5.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Theo Sanderson</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Theo Sanderson</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Adventures with InfoGANs: towards generative models of biological images (part 1)</h1><div class=article-metadata><span class=article-date>Oct 2, 2018</span>
<span class=middot-divider></span><span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/category/ml/>ML</a></span></div></div><link rel=stylesheet href=/theo.css><div class=article-container><div class=article-style><p>I recently began an <a href=https://ai.google/research/join-us/ai-residency/ target=_blank rel=noopener>AI Residency</a> at Google, which I am enjoying a great deal. I have been experimenting with deep-learning approaches for a few years now, but am excited to immerse myself in this world over the coming year. Biology increasingly generates very large datasets and I am convinced that novel machine-learning approaches will be essential to make the most of them.</p><p>At the beginning of my residency, I was advised to complete a <em>mini-project</em> which largely reimplements existing work, as an introduction to new tools.  In this post I’m going to describe what I got up to during that first few weeks, which culminated in the tool below that conjures up new images of red blood cells infected with malaria parasites:</p><p><script>function resizeIframe(obj){obj.style.height=obj.contentWindow.document.body.scrollHeight+'px';}</script><br><iframe src=http://static.theo.io/GAN/plasmodium_gan/test.html style=width:100%;height:500px frameborder=0 scrolling=no onload=resizeIframe(this)></iframe></p><p>I’m going to try to explain it from first principles, which might take a while, so if you have a background in machine-learning, <a href=http://theo.io/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images-part-2/ target=_blank rel=noopener>skip to the next post</a>.</p><p><strong>Neural net-what?</strong></p><p>Deep learning is based around things called neural networks. A neural network takes a series of numbers in, does some processing to them, and ultimately spits out another set of numbers. A system like this is in theory capable of undertaking a great many tasks: for example the first series of numbers could be the brightness values for each pixel in an image, and the number outputted could be the probability that the image contains a cat. If the network can map those sets of numbers to each other successfully, then you have a cat-recognition network.</p><figure id=attachment_221 aria-describedby=caption-attachment-221 style=width:833px class="wp-caption aligncenter"><img src=/post/archive_posts/old_wp_images/wp-content/uploads/2018/09/cat-01.png alt width=833 height=342 class="size-full wp-image-221" srcset="/post/archive_posts/old_wp_images/wp-content/uploads/2018/09/cat-01.png 833w, /post/archive_posts/old_wp_images/wp-content/uploads/2018/09/cat-01-300x123.png 300w, /post/archive_posts/old_wp_images/wp-content/uploads/2018/09/cat-01-768x315.png 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px"><figcaption id=caption-attachment-221 class=wp-caption-text>A hypothetical classification network to detect images of cats</figcaption></figure><p>But what determines whether the network will recognise cats,  koalas, or fail to recognise anything at all? Each network contains a vast array of <em>parameters</em>, which determine how the input numbers are processed to produce the output. These are knobs which can be twiddled to make the network do different things.</p><p>The magic that makes neural networks powerful is a process called <em>back-propagation</em>, which allows the system to automatically twiddle these knobs until the network produces the desired output for each input. This requires a large amount of labelled data. For our cat network we might give it 1,000 photos of cats, and 1,000 photos of dogs. The system will start off by setting the parameters to random values. It will then feed in the first cat photo and see what number it gets out. With these random values, the network will most likely output 0.5 as the cat probability, even though the true value is 1 (it’s a cat!). But with back-propagation the system can observe this error (0.5 – 1 = -0.5) and calculate which direction it needs to turn each knob (and by how much) to make the error smaller. If we repeat this process over and over and over again (which is called <em>training</em>), we will eventually we find we have a network that can reliably distinguish cats and dogs. And it turns out that the graphics cards in our computers can be commandeered to make that process happen pretty quickly. In an hour or so we can have our cat recognition network – jubilations.</p><p>With those basic principles one can build a network to do any sort of classification problem. If you have searched your own photos based on what they contain  (e.g. on Google photos) then you have interacted with just such a network.</p><p><strong>Making computers creative</strong></p><p>You might think at this point that our network has a pretty good idea what a cat looks like, but unfortunately with this approach you can’t ask it to draw one.</p><p>One <em>can</em> imagine a neural network which could draw, however: this time the <em>output</em> would be a series of numbers, which we will convert into to the brightness values of pixels in an image. What should the input be? It turns out we can just use random numbers:  for each image we can sample say 10 values from a normal distribution, and feed them in. That way, in theory, the network can draw on this randomness to generate a different picture each time, and act as an image <em>generator</em>.</p><p>There is a problem of course. The network still has no idea what cats look like. When it is created, with its knobs set to random positions, it will generate images that resemble those on an un-tuned TV. In theory we could train it ourselves, by letting it know whenever it produced noise that had the faintest of resemblance to a cat, until it was coaxed into doing what we wanted. In practice however, this would take an eternity.</p><p>So what to do? Well, there’s a trick, <a href=https://arxiv.org/abs/1406.2661 target=_blank rel=noopener>invented</a> just four years ago, which has been revolutionary. We can introduce a second network, called a <em>discriminator</em>. This is a classification network, like the one we imagined above. But this time its task is not to distinguish cats from dogs, but to distinguish real photos of actual cats, from fake cats imagined by the generator. We feed it some real cats, and some fake ‘cats’ that the generator has produced, and we train it. Initially it has a very easy job here, since remember the generator is just displaying the images of an untuned TV. But wait..</p><p>Once the discriminator starts to do a decent job we can take our <em>back-propagation</em> one step further. The system can backpropagate from the final result (“fake” or “real”) back to the pixel values produced by the generator, and then all the way back through the generator to the random variables that made these fake images. Then it can ask <em>“what directions should I turn the knobs of the generator so that it produces an output that the discriminator thinks is real?”</em>. In this way, the generator can be trained to fool the discriminator, and in doing so, the images it produces will become a bit more cat-like.</p><p>The discriminator can now be trained again, to be more expert in telling these synthetic cats from their real counterparts. In fact we bounce back and forth, training generator and then discriminator in a continual loop. They have opposing goals, the generator wants to make realistic cat images and the discriminator wants to tell these apart from <em>true</em> cat images, and so this architecture is called a <em>generative adversarial network</em> or GAN. After a long period of training these networks can produce  results like these:</p><figure id=attachment_180 aria-describedby=caption-attachment-180 style=width:288px class="wp-caption alignnone"><img class=wp-image-180 src=/post/archive_posts/old_wp_images/wp-content/uploads/2018/08/Screenshot-from-2018-08-27-21-19-48.png alt width=288 height=293 srcset="/post/archive_posts/old_wp_images/wp-content/uploads/2018/08/Screenshot-from-2018-08-27-21-19-48.png 663w, /post/archive_posts/old_wp_images/wp-content/uploads/2018/08/Screenshot-from-2018-08-27-21-19-48-296x300.png 296w" sizes="(max-width: 288px) 85vw, 288px"><figcaption id=caption-attachment-180 class=wp-caption-text>Results of a cat GAN. (This figure comes from a [paper][4] by Alexia Jolicoeur-Martinea about a novel type of discriminator)</figcaption></figure><figure id=attachment_179 aria-describedby=caption-attachment-179 style=width:512px class="wp-caption alignnone"><img class="wp-image-179 size-full" src=/post/archive_posts/old_wp_images/wp-content/uploads/2018/08/representative_image_512x256.png alt width=512 height=256 srcset="/post/archive_posts/old_wp_images/wp-content/uploads/2018/08/representative_image_512x256.png 512w, /post/archive_posts/old_wp_images/wp-content/uploads/2018/08/representative_image_512x256-300x150.png 300w" sizes="(max-width: 512px) 85vw, 512px"><figcaption id=caption-attachment-179 class=wp-caption-text>And with some extensions of the GAN approach [Nvidia][5] were able to hallucinate these wholly imaginary celebrities.</figcaption></figure><p>Not bad, eh?</p><p><strong>InfoGAN – images that communicate information</strong></p><p>This approach can produce realistic images. But the relationship between the random noise at the beginning and the image at the end is not generally clear. If we increase value 3 by 50% what will happen? It is generally difficult to predict, and does not correspond in a useful way to a semantically meaningful property of the image.</p><p>There would be a lot of value to a network which, without being specifically trained to, could actually understand the structure in these images (e.g. that there are different breeds of cat) and that at the end could generate an image of any breed of your choice.</p><p>Some people came up with a very clever way of doing this, called an InfoGAN. This approach is very similar to the original GAN. The generator still draws on random noise to produce its image. However the generator is also given some <em>extra noise</em>, which it is tasked with encoding into the image it produces. For instance in the original InfoGAN paper the researchers produced a network which made images of hand-written digits. The extra noise they added was a discrete ‘one-hot’ variable with ten possible values. The idea was to create a network where each of these values corresponded to a different digit. To get this to happen they created an additional training objective for both the generator and the discriminator. As before, the discriminator still wants to distinguish real images from fake images and the generator still wants to fool the discriminator. However <em>both the discriminator and the generator</em> are also rewarded if the discriminator is able to successfully reconstruct the extra random variable that was fed to the generator.</p><p>This means the generator now has to encode <em>information</em> (a variable with ten possible values) into an image, but that image has to be a plausible member of the set of hand-written digit images. The strategy that it ends up taking is to encode each value as a separate digit, and tada, we have achieved our goal. We can now ask the network to generate any specific digit we like. We can choose threes and generate an infinite number of different threes in different styles. The researchers also showed that if they added two more continuous variables to be communicated, these ended up mapped to the angle, and the width, of the digit produced.</p><p> </p><figure id=attachment_190 aria-describedby=caption-attachment-190 style=width:639px class="wp-caption alignnone"><img class="wp-image-190 size-full" src=/post/archive_posts/old_wp_images/wp-content/uploads/2018/09/1_kyyjNnuNaOscjucBpql2AA.png alt width=639 height=390 srcset="/post/archive_posts/old_wp_images/wp-content/uploads/2018/09/1_kyyjNnuNaOscjucBpql2AA.png 639w, /post/archive_posts/old_wp_images/wp-content/uploads/2018/09/1_kyyjNnuNaOscjucBpql2AA-300x183.png 300w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px"><figcaption id=caption-attachment-190 class=wp-caption-text>Figure from the InfoGAN paper showing the three properties the network has learnt (digit type, angle and width)</figcaption></figure><p>I began by reimplementing some of what these researchers had achieved, focusing on this hand-written digit task. Here are the images my network produced evolving over time:</p><p><img src=/post/archive_posts/old_wp_images/wp-content/uploads/2018/09/image3.gif alt width=140 height=168 class="aligncenter size-full wp-image-223"></p><p>You can see the network gradually deciding how it will encode each value. It is undecided about what should be a five and what should be a zero for a long time, but it eventually comes to a conclusion. So without labelling any of the data, the system has learnt to partition it into ten appropriate categories, and has built a system for arbitrarily generation members of each of these categories.</p><p>There are a near-infinite number of possible threes that can be drawn – cleaner or curlier, thicker or thinner, but all are united in being threes. I think that a very similar property applies in biological cells and in the <a href=http://theo.io/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images-part-2/ target=_blank rel=noopener>next post</a> I’ll describe my forays into this area, and the promise I think such an approach holds.</p><p><a href=http://theo.io/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images-part-2/ target=_blank rel=noopener>&#187;&#187; Part 2</a></p></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images/&text=Adventures%20with%20InfoGANs:%20towards%20generative%20models%20of%20biological%20images%20%28part%201%29" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images/&t=Adventures%20with%20InfoGANs:%20towards%20generative%20models%20of%20biological%20images%20%28part%201%29" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Adventures%20with%20InfoGANs:%20towards%20generative%20models%20of%20biological%20images%20%28part%201%29&body=/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images/&title=Adventures%20with%20InfoGANs:%20towards%20generative%20models%20of%20biological%20images%20%28part%201%29" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Adventures%20with%20InfoGANs:%20towards%20generative%20models%20of%20biological%20images%20%28part%201%29%20/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images/&title=Adventures%20with%20InfoGANs:%20towards%20generative%20models%20of%20biological%20images%20%28part%201%29" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu8af67025a90ae4f28f8d45441763eced_96138_270x270_fill_q75_lanczos_center.jpg alt="Theo Sanderson"><div class=media-body><h5 class=card-title>Theo Sanderson</h5><h6 class=card-subtitle>Sir Henry Wellcome Fellow</h6><p class=card-text>Biologist developing tools to scale malaria reverse genetics.</p><ul class=network-icon aria-hidden=true><li><a href=/#contact><i class="fas fa-envelope"></i></a></li><li><a href=https://twitter.com/theosanderson target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=voDDwYIAAAAJ" target=_blank rel=noopener><i class="fas fa-graduation-cap"></i></a></li><li><a href=https://github.com/theosanderson target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/blog/2018/11/13/biggan-interpolations/>BigGAN interpolations</a></li><li><a href=/blog/2018/10/02/adventures-with-infogans-towards-generative-models-of-biological-images-part-2/>Adventures with InfoGANs: towards generative models of biological images (part 2)</a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
        <div class="search-hit-content">
          <div class="search-hit-name">
            <a href="{{relpermalink}}">{{title}}</a>
            <div class="article-metadata search-hit-type">{{type}}</div>
            <p class="search-hit-description">{{snippet}}</p>
          </div>
        </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/en/js/wowchemy.min.297f736e130400781cfee00eff1f1afd.js></script></body></html>